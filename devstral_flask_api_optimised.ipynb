{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8212d982-1652-4622-bbe0-6f6ae4e8311f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Alta_doc_prompt.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    prompt = f.read()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "142e7dac-46fb-42fa-9218-6c51b636eaa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5000\n",
      " * Running on http://10.192.26.140:5000\n",
      "Press CTRL+C to quit\n",
      "10.192.26.140 - - [16/Jun/2025 05:12:33] \"GET / HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "import threading\n",
    "import requests\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/')\n",
    "def home():\n",
    "    return '‚úÖ L‚ÄôAPI fonctionne ! Utilise POST sur /document pour envoyer du code.'\n",
    "\n",
    "# üìÑ Route pour envoyer du code √† documenter\n",
    "@app.route('/document', methods=['POST'])\n",
    "def document_code():\n",
    "    data = request.get_json()\n",
    "    code = data.get('code', '')\n",
    "\n",
    "    if not code:\n",
    "        return jsonify({'error': 'No code provided'}), 400\n",
    "\n",
    "    \n",
    "    try:\n",
    "        with open(\"Alta_doc_prompt.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "            base_prompt = f.read()\n",
    "    except FileNotFoundError:\n",
    "        return jsonify({'error': 'Prompt file not found'}), 500\n",
    "\n",
    "    full_prompt = f\"{base_prompt}\\n\\n{code}\"\n",
    "\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            'http://localhost:11434/api/generate',\n",
    "            json={\n",
    "                \"model\": \"devstral:24b\",  \n",
    "                \"prompt\": full_prompt,\n",
    "                \"stream\": False\n",
    "            }\n",
    "        )\n",
    "        return jsonify({'documentation': response.json().get('response', '')})\n",
    "    except Exception as e:\n",
    "        return jsonify({'error': str(e)}), 500\n",
    "\n",
    "# üöÄ Lancer le serveur Flask en arri√®re-plan\n",
    "def run_app():\n",
    "    app.run(host='0.0.0.0', port=5000)\n",
    "\n",
    "threading.Thread(target=run_app).start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f2ff4fe-2485-4c45-86b0-b758dc8aa5fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"error\":\"model 'devstral:24b' not found\"}\n",
      "‚ùå Aucune r√©ponse g√©n√©r√©e\n"
     ]
    }
   ],
   "source": [
    "code_content = \"\"\"\n",
    "# script for training the model\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "torch.backends.cudnn.enabled = False\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "ROOT = Path(os.path.abspath(__file__)).parents[2]\n",
    "sys.path.append(str(ROOT))\n",
    "\n",
    "from src.unet.dataset.dataset import SegrailsDataset\n",
    "from src.unet.model.losses import Loss\n",
    "from src.unet.model.optimizers import Optimizer, Scheduler\n",
    "from src.unet.model.unet import Unet\n",
    "from src.unet.trainer.trainer import Trainer\n",
    "from src.unet.utils.parameters import train_parameters\n",
    "from src.unet.utils.strings import dict2print, get_logger\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--train_data\", type=str, help=\"path to train data\")\n",
    "    parser.add_argument(\"--val_data\", type=str, help=\"path to validation data\")\n",
    "    parser.add_argument(\"--log_dir\", type=str, default=\"../../runs\", help=\"path to log\")\n",
    "    parser.add_argument(\"--loss\", type=str, help=\"loss function\")\n",
    "    parser.add_argument(\"--optimizer\", type=str, help=\"optimizer function\")\n",
    "    parser.add_argument(\"--lr\", type=float, help=\"learning rate\")\n",
    "    parser.add_argument(\"--weight_decay\", type=str, help=\"weight decay\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    tuning_parameters = {\n",
    "        \"loss\": str(args.loss),\n",
    "        \"optimizer\": {\n",
    "            \"method\": str(args.optimizer),\n",
    "            \"lr\": float(args.lr),\n",
    "            \"momentum\": 0.8,\n",
    "            \"weight_decay\": float(args.weight_decay),\n",
    "        },\n",
    "    }\n",
    "\n",
    "    config = train_parameters()\n",
    "    model_params = config[\"model\"]\n",
    "    train_params = config[\"train\"]\n",
    "    eval_params = config[\"evaluate\"]\n",
    "    train_params.update(tuning_parameters)\n",
    "\n",
    "    if not os.path.exists(args.log_dir):\n",
    "        os.mkdir(args.log_dir)\n",
    "    log_filename = os.path.join(args.log_dir, \"train.log\")\n",
    "    logger = get_logger(log_filename, \"trainer\", True)\n",
    "    logger.info(\"\\\\nNew Experiment\\\\n\")\n",
    "\n",
    "    model = Unet(**model_params)\n",
    "    logger.info(\"Successfully created the model with the following parameters:\")\n",
    "    logger.info(dict2print(model_params))\n",
    "    logger.info(f\"Training on {train_params['epochs']} epochs\")\n",
    "\n",
    "    checkpoint_file = os.path.join(args.log_dir, \"best.pt\")\n",
    "\n",
    "    im_size = train_params[\"image_size\"]\n",
    "    logger.info(f\"Images are resized to {str(im_size)}\")\n",
    "    transforms = v2.Compose([v2.RandomCrop(size=im_size)])\n",
    "    train_dataset = SegrailsDataset(args.train_data, transforms=transforms)\n",
    "    val_dataset = SegrailsDataset(args.val_data, transforms=transforms)\n",
    "    logger.info(f\"Loaded the train dataset: {len(train_dataset)} images\")\n",
    "    logger.info(f\"Loaded the validation dataset: {len(val_dataset)} images\")\n",
    "    logger.info(\"\\\\n\")\n",
    "\n",
    "    optimizer = Optimizer(**train_params[\"optimizer\"]).attach(model)\n",
    "    scheduler = Scheduler(**train_params[\"scheduler\"]).attach(optimizer)\n",
    "    loss_fn = Loss(train_params[\"loss\"], True).func()\n",
    "\n",
    "    trainer = Trainer(\n",
    "        logger,\n",
    "        train_dataset,\n",
    "        val_dataset,\n",
    "        model,\n",
    "        loss_fn,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        train_params,\n",
    "        eval_params,\n",
    "        checkpoint_file,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\"\"\"\n",
    "\n",
    "with open(\"train_tuning.py\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(code_content)\n",
    "\n",
    "print(\"‚úÖ Fichier 'train_tuning.py' enregistr√© avec succ√®s.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5f8301-28ee-4d26-b71a-3f97649a05ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
